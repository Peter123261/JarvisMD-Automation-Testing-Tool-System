services:
  # PostgreSQL Database
  postgres:
    image: postgres:16-alpine
    container_name: medbench-postgres
    environment:
      POSTGRES_DB: ${POSTGRES_DB:-jarvismd}
      POSTGRES_USER: ${POSTGRES_USER:-medbench}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-medbench_secure_password}
    ports:
      - "${POSTGRES_PORT:-5432}:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./docker/init-scripts:/docker-entrypoint-initdb.d
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-medbench} -d ${POSTGRES_DB:-jarvismd}"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - medbench-network
    restart: unless-stopped

  # Redis Cache and Message Broker
  redis:
    image: redis:7-alpine
    container_name: medbench-redis
    command: redis-server --appendonly yes
    ports:
      - "${REDIS_PORT:-6379}:6379"
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - medbench-network
    restart: unless-stopped

  # FastAPI Application
  api:
    build:
      context: .
      dockerfile: Dockerfile
      target: development
    container_name: medbench-api
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - DATABASE_URL=postgresql://${POSTGRES_USER:-medbench}:${POSTGRES_PASSWORD:-medbench_secure_password}@postgres:5432/${POSTGRES_DB:-jarvismd}
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_DB=0
      - BACKEND_HOST=0.0.0.0
      - BACKEND_PORT=8000
      - DEBUG=true
      - LOG_LEVEL=INFO
      - CORS_ORIGINS=${CORS_ORIGINS}
      - USE_CELERY_BATCH=true
      - API_METRICS_PORT=${API_METRICS_PORT:-8007}
      - WORKER_METRICS_PORT=${WORKER_METRICS_PORT:-8002}
    ports:
      - "${API_PORT:-8000}:8000"
      - "${API_METRICS_PORT:-8007}:${API_METRICS_PORT:-8007}"
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    volumes:
      - ./jarvismd/data/prompts:/app/jarvismd/data/prompts
      - ./jarvismd/data/medical_cases:/app/jarvismd/data/medical_cases
    networks:
      - medbench-network
    restart: unless-stopped

  # Celery Worker
  celery-worker:
    build:
      context: .
      dockerfile: Dockerfile
      target: development
    container_name: medbench-celery-worker
    command: celery -A jarvismd.backend.automation.task_queue.celery_app worker --loglevel=info --pool=threads --concurrency=4
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - DATABASE_URL=postgresql://${POSTGRES_USER:-medbench}:${POSTGRES_PASSWORD:-medbench_secure_password}@postgres:5432/${POSTGRES_DB:-jarvismd}
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_DB=0
      - LOG_LEVEL=INFO
      - CORS_ORIGINS=${CORS_ORIGINS}
      - WORKER_METRICS_PORT=${WORKER_METRICS_PORT:-8002}
      - API_METRICS_PORT=${API_METRICS_PORT:-8007}
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    volumes:
      - ./jarvismd/data/prompts:/app/jarvismd/data/prompts
      - ./jarvismd/data/medical_cases:/app/jarvismd/data/medical_cases
    networks:
      - medbench-network
    restart: on-failure
    deploy:
      restart_policy:
        condition: on-failure
        max_attempts: 1
    ports:
      - "${WORKER_METRICS_EXTERNAL_PORT:-8006}:${WORKER_METRICS_PORT:-8002}"

  # Celery Beat (Scheduler)
  celery-beat:
    build:
      context: .
      dockerfile: Dockerfile
      target: development
    container_name: medbench-celery-beat
    command: celery -A jarvismd.backend.automation.task_queue.celery_app beat --loglevel=info
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - DATABASE_URL=postgresql://${POSTGRES_USER:-medbench}:${POSTGRES_PASSWORD:-medbench_secure_password}@postgres:5432/${POSTGRES_DB:-jarvismd}
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_DB=0
      - LOG_LEVEL=INFO
      - CORS_ORIGINS=${CORS_ORIGINS}
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - medbench-network
    restart: on-failure
    deploy:
      restart_policy:
        condition: on-failure
        max_attempts: 1

  # Flower (Celery Monitoring)
  flower:
    build:
      context: .
      dockerfile: Dockerfile
      target: development
    container_name: medbench-flower
    command: celery -A jarvismd.backend.automation.task_queue.celery_app flower --port=5555
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - DATABASE_URL=postgresql://${POSTGRES_USER:-medbench}:${POSTGRES_PASSWORD:-medbench_secure_password}@postgres:5432/${POSTGRES_DB:-jarvismd}
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_DB=0
      - LOG_LEVEL=INFO
      - CORS_ORIGINS=${CORS_ORIGINS}
      - FLOWER_UNAUTHENTICATED_API=true
    ports:
      - "${FLOWER_PORT:-5555}:5555"
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - medbench-network
    restart: on-failure
    deploy:
      restart_policy:
        condition: on-failure
        max_attempts: 1

  # Prometheus - Metrics collection and storage
  prometheus:
    image: prom/prometheus:latest
    container_name: medbench-prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=7d'
      - '--web.enable-lifecycle'
    ports:
      # Expose Prometheus to LAN/any host
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - ./monitoring/alert_rules.yml:/etc/prometheus/alert_rules.yml
      - prometheus_data:/prometheus
    networks:
      - medbench-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "-qO-", "http://localhost:9090/-/ready"]
      interval: 30s
      timeout: 5s
      retries: 5

  grafana:
    image: grafana/grafana:10.4.1
    container_name: medbench-grafana
    ports:
      # Expose Grafana to LAN/any host so teammates can access
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD:-admin}
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/provisioning/datasources:/etc/grafana/provisioning/datasources
      - ./monitoring/grafana/provisioning/dashboards:/etc/grafana/provisioning/dashboards
      - ./monitoring/grafana/dashboards:/var/lib/grafana/dashboards
    depends_on:
      - prometheus
    networks:
      - medbench-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "-qO-", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 5s
      retries: 5

  alertmanager:
    image: prom/alertmanager:latest
    container_name: medbench-alertmanager
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
      - '--log.level=info'
    ports:
      # Expose Alertmanager to LAN/any host
      - "9093:9093"
    volumes:
      - ./monitoring/alertmanager.yml:/etc/alertmanager/alertmanager.yml
      - alertmanager_data:/alertmanager
    networks:
      - medbench-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "-qO-", "http://localhost:9093/-/ready"]
      interval: 30s
      timeout: 5s
      retries: 5

  # node_exporter - Host metrics
  node-exporter:
    image: prom/node-exporter:latest
    container_name: medbench-node-exporter
    pid: "host"
    ports:
      - "9100:9100"
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.sysfs=/host/sys'
      - '--path.rootfs=/rootfs'
      - '--collector.filesystem.ignored-mount-points=^/(dev|proc|sys|var/lib/docker/.+)($|/)'
    networks:
      - medbench-network
    restart: unless-stopped

  # cAdvisor - Container metrics
  cadvisor:
    image: gcr.io/cadvisor/cadvisor:latest
    container_name: medbench-cadvisor
    privileged: true
    devices:
      - /dev/kmsg
    ports:
      - "8080:8080"
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
      - /cgroup:/cgroup:ro
    networks:
      - medbench-network
    restart: unless-stopped

  # OpenTelemetry Collector
  otel-collector:
    image: otel/opentelemetry-collector:latest
    container_name: medbench-otel-collector
    command: ["--config=/etc/otelcol/config.yaml"]
    volumes:
      - ./monitoring/otel-collector-config.yml:/etc/otelcol/config.yaml
    ports:
      - "4317:4317"  # OTLP gRPC
      - "4318:4318"  # OTLP HTTP
      - "8889:8889"  # Prometheus metrics endpoint
    depends_on:
      - tempo
    networks:
      - medbench-network
    restart: unless-stopped
    # Health check removed - otel-collector container may not have wget/curl utilities
    # Collector is working correctly (verified in logs), health check is optional

  # Tempo (Trace storage)
  tempo:
    image: grafana/tempo:latest
    container_name: medbench-tempo
    command: ["-config.file=/etc/tempo/tempo.yml"]
    volumes:
      - ./monitoring/tempo-config.yml:/etc/tempo/tempo.yml
      - tempo_data:/var/tempo
    ports:
      - "3200:3200"  # Tempo
    networks:
      - medbench-network
    restart: unless-stopped
    # Health check removed - Tempo container doesn't have wget/curl/nc utilities
    # Tempo is working correctly (verified in logs), health check is optional

networks:
  medbench-network:
    driver: bridge

volumes:
  postgres_data:
  redis_data:
  prometheus_data:
  alertmanager_data:
  grafana_data:
  tempo_data:

